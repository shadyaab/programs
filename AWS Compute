Instance Purchasing Options:
---------------------------
On-Demand Instances 
  – With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. 
  - You have full control over the instance's lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it.
  - There is no long-term commitment required when you purchase On-Demand Instances. 

Reserved Instances 
  – Reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, 
    for a term of 1 or 3 years.
  - Reserved Instances provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. 

  Payment options
    - All Upfront: Full payment is made at the start of the term, with no other costs or additional hourly charges incurred for the remainder of the term, regardless of hours used.
    - Partial Upfront: A portion of the cost must be paid upfront and the remaining hours in the term are billed at a discounted hourly rate, regardless of whether the Reserved Instance is being used.
    - No Upfront: You are billed a discounted hourly rate for every hour within the term, regardless of whether the Reserved Instance is being used. No upfront payment is required.

Scheduled Reserved Instances
  - With Scheduled Reserved Instances, you can reserve capacity that is scheduled to recur daily, weekly, or monthly, 
    with a specified start time and duration, for a one-year term. After you complete your purchase, the instances are available to launch 
    during the time windows that you specified.

Spot Instances 
  – Request unused EC2 instances, which can reduce your Amazon EC2 costs significantly.
  - A Spot Instance is an instance that uses spare EC2 capacity that is available for less than the On-Demand price
  - Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.
  - Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks. 
  - Amazon EC2 provides a Spot Instance interruption notice, which gives the instance a two-minute warning before it is interrupted.

Dedicated Instances 
  - Dedicated Instances are EC2 instances that run on hardware that's dedicated to a single customer.
  – Pay, by the hour, for instances that run on single-tenant hardware.

Dedicated Hosts 
  - Same as Dedicated instance
  - A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server
  – Pay for a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-VM software licenses to reduce costs.
  
On Demand Capacity Reservations 
  – On-Demand Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone for any duration.
  - Capacity Reservations mitigate against the risk of being unable to get On-Demand capacity in case there are capacity constraints
  
AWS Elastic Beanstalk
----------------------
With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure 
that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, 
and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.
  
  
Lambda
-------
AWS Lambda is a compute service that lets you run code without provisioning or managing servers.
Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, 
including server and operating system maintenance, capacity provisioning and automatic scaling, and logging. With Lambda, 
all you need to do is supply your code in one of the language runtimes that Lambda supports.
  
Concpets:  
  Function 
    - A function is a resource that you can invoke to run your code in Lambda. 
    - A function has code to process the events that you pass into the function or that other AWS services send to the function.

  Trigger
    - A trigger is a resource or configuration that invokes a Lambda function.
    - triggers include AWS services that you can configure to invoke a function and event source mappings (An event source mapping 
      is a resource in Lambda that reads items from a stream or queue and invokes a function.)

  Event
    - An event is a JSON-formatted document that contains data for a Lambda function to process
    - The runtime converts the event to an object and passes it to your function code.

  Execution environment
    - An execution environment provides a secure and isolated runtime environment for your Lambda function
    - An execution environment manages the processes and resources that are required to run the function.

  Instruction set architecture
    - The instruction set architecture determines the type of computer processor that Lambda uses to run the function

  Deployment package
    - You deploy your Lambda function code using a deployment package. 
    - Supports both zip file and Container Image

  Runtime
    - The runtime provides a language-specific environment that runs in an execution environmen
    - If you package your code as a .zip file archive, you must configure your function to use a runtime that matches your programming language.
    -  For a container image, you include the runtime when you build the image.

  Layer
    - A Lambda layer is a .zip file archive that can contain additional code or other content. A layer can contain libraries, a custom runtime, 
      data, or configuration files.
    - Layers also promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic.
    - Supports upto five layers
    - Functions deployed as a container image do not use layers.

  Concurrency
    - Concurrency is the number of requests that your function is serving at any given time

  Qualifier
    - When you invoke or view a function, you can include a qualifier to specify a version or alias
    - You can use versions and aliases together to provide a stable interface for clients to invoke your function.

  Destination
    - A destination is an AWS resource where Lambda can send events from an asynchronous invocation.
    - You can configure a destination for events that fail processing.
  
Invoking Function:  
  Synchronous invocation
    - When you invoke a function synchronously, Lambda runs the function and waits for a response.
    - when the function completes, Lambda returns the response from the function's code with additional data, such as the version of the function 
      that was invoked. 
  
  Asynchronous invocation
    - Several AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke functions asynchronously to process events.
    - Lambda queues the events before sending them to the function.
    - A separate process reads events from the queue and sends them to your function
    - To invoke a function asynchronously, set the invocation type parameter to Event.
    - Lambda manages the function's asynchronous event queue and attempts to retry on errors.
    - If the function doesn't have enough concurrency available to process all events, additional requests are throttled
    - You can configure separate destinations for events that are processed successfully, and events that fail all processing attempts
    - you can configure a standard Amazon SQS queue or standard Amazon SNS topic as a dead-letter queue for discarded events
    
  Event source mappings
    - An event source mapping is a Lambda resource that reads from an event source and invokes a Lambda function
    - You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly.
    
      
AWS Batch
---------
AWS Batch helps you to run batch computing workloads on the AWS Cloud. Batch computing is a common way for developers, scientists, 
and engineers to access large amounts of compute resources. AWS Batch removes the undifferentiated heavy lifting of configuring and 
managing the required infrastructure, similar to traditional batch computing software. This service can efficiently provision resources 
in response to jobs submitted in order to eliminate capacity constraints, reduce compute costs, and deliver results quickly.

Components of AWS Batch
  Jobs
    - A unit of work (such as a shell script, a Linux executable, or a Docker container image) that you submit to AWS Batch.
    - It has a name, and runs as a containerized application on AWS Fargate or Amazon EC2 resources in your compute environment, 
      using parameters that you specify in a job definition.
    - Jobs can reference other jobs by name or by ID
    
  Job Definitions
    - A job definition specifies how jobs are to be run.
    - You can think of a job definition as a blueprint for the resources in your job.
    - You can supply your job with an IAM role to provide access to other AWS resources. You also specify both memory and CPU requirements. 
    - While each job must reference a job definition, many of the parameters that are specified in the job definition can be overridden at runtime.
    
  Job Queues
    - When you submit an AWS Batch job, you submit it to a particular job queue, where the job resides until it's scheduled onto a compute environment.
    - You associate one or more compute environments with a job queue.
    - You can also assign priority values for these compute environments and even across job queues themselves.
    
  Compute Environment
    - A compute environment is a set of managed or unmanaged compute resources that are used to run jobs
    - With managed compute environments, you can specify desired compute type (Fargate or EC2) at several levels of detail.
    - With Unmanaged compute environments, you're responsible for setting up and scaling the instances in an Amazon ECS cluster that AWS Batch 
      creates for you.

AWS Outposts
--------------
AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. By providing 
local access to AWS managed infrastructure, AWS Outposts enables customers to build and run applications on premises using the same 
programming interfaces as in AWS Regions, while using local compute and storage resources for lower latency and local data processing needs.

An Outpost is a pool of AWS compute and storage capacity deployed at a customer site. AWS operates, monitors, and manages this capacity as 
part of an AWS Region. You can create subnets on your Outpost and specify them when you create AWS resources such as EC2 instances, 
EBS volumes, ECS clusters, and RDS instances. Instances in Outpost subnets communicate with other instances in the AWS Region using 
private IP addresses, all within the same VPC.


ECR
-----
Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable. 
Amazon ECR supports private repositories with resource-based permissions using AWS IAM. This is so that specified users or Amazon EC2 instances 
can access your container repositories and images. You can use your preferred CLI to push, pull, and manage Docker images, Open Container 
Initiative (OCI) images, and OCI compatible artifacts.

Components of Amazon ECR
  Registry
    - An Amazon ECR private registry is provided to each AWS account; you can create one or more repositories in your registry and store images in them. 
    
  Authorization token
    - Your client must authenticate to Amazon ECR registries as an AWS user before it can push and pull images.
  
  Repository
    - An Amazon ECR repository contains your Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts. 
  
  Repository policy
    - You can control access to your repositories and the images within them with repository policies. 
  
  Image
    - You can push and pull container images to your repositories. You can use these images locally on your development system, 
      or you can use them in Amazon ECS task definitions and Amazon EKS pod specifications.

ECS
-------

EKS
-----
Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, 
operate, and maintain your own Kubernetes control plane or nodes.

Feature:
  - Runs and scales the Kubernetes control plane across multiple AWS Availability Zones to ensure high availability.
  - Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instances, and it provides automated version 
    updates and patching for them.
  - Is integrated with many AWS services to provide scalability and security for your applications
    - Amazon ECR for container images
    - Elastic Load Balancing for load distribution
    - IAM for authentication
    - Amazon VPC for isolation
  -  Amazon EKS are fully compatible with applications running on any standard Kubernetes environment  
  - Amazon EKS runs a single tenant Kubernetes control plane for each cluster.
  - The control plane infrastructure isn't shared across clusters or AWS accounts.
  - The control plane consists of at least two API server instances and three etcd instances that run across three Availability Zones within an AWS Region
  - Amazon EKS uses Amazon VPC network policies to restrict traffic between control plane components to within a single cluster. 
  - All of the data stored by the etcd nodes and associated Amazon EBS volumes is encrypted using AWS KMS.

Control Plane
  - The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server.
  - The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the Amazon EKS endpoint associated with your cluster
  - Each Amazon EKS cluster control plane is single-tenant and unique, and runs on its own set of Amazon EC2 instances.


  
  
  
  

    
